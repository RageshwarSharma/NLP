{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given two paragraphs, quantify the degree of similarity between the two text-based on Semantic similarity. Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other. The STS task is motivated by the observation that accurately modelling the meaning similarity of sentences is a foundational language understanding problem relevant to numerous applications including machine translation (MT), summarization, generation, question-answering (QA), short answer grading, semantic search.  \n",
    "STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The data contains a pair of paragraphs. These text paragraphs are randomly sampled from a raw dataset. Each pair of the sentence may or may not be semantically similar. The candidate is to predict a value between 0-1 indicating a degree of similarity between the pair of text paras.  \n",
    "1 means highly similar  \n",
    "0 means highly dissimilar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "wordnet=WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pickle\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('D:\\\\python\\\\Text_Similarity_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>labour plans maternity pay rise maternity pay ...</td>\n",
       "      <td>no seasonal lift for house market a swathe of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>high fuel costs hit us airlines two of the lar...</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>britons growing  digitally obese  gadget lover...</td>\n",
       "      <td>film star fox behind theatre bid leading actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>tsunami  to hit sri lanka banks  sri lanka s b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>nuclear dumpsite  plan attacked plans to allow...</td>\n",
       "      <td>x factor show gets second series tv talent sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID                                              text1  \\\n",
       "0             0  savvy searchers fail to spot ads internet sear...   \n",
       "1             1  millions to miss out on the net by 2025  40% o...   \n",
       "2             2  young debut cut short by ginepri fifteen-year-...   \n",
       "3             3  diageo to buy us wine firm diageo  the world s...   \n",
       "4             4  be careful how you code a new european directi...   \n",
       "...         ...                                                ...   \n",
       "4018       4018  labour plans maternity pay rise maternity pay ...   \n",
       "4019       4019  high fuel costs hit us airlines two of the lar...   \n",
       "4020       4020  britons growing  digitally obese  gadget lover...   \n",
       "4021       4021  holmes is hit by hamstring injury kelly holmes...   \n",
       "4022       4022  nuclear dumpsite  plan attacked plans to allow...   \n",
       "\n",
       "                                                  text2  \n",
       "0     newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1     nasdaq planning $100m share sale the owner of ...  \n",
       "2     ruddock backs yapp s credentials wales coach m...  \n",
       "3     mci shares climb on takeover bid shares in us ...  \n",
       "4     media gadgets get moving pocket-sized devices ...  \n",
       "...                                                 ...  \n",
       "4018  no seasonal lift for house market a swathe of ...  \n",
       "4019  new media battle for bafta awards the bbc lead...  \n",
       "4020  film star fox behind theatre bid leading actor...  \n",
       "4021  tsunami  to hit sri lanka banks  sri lanka s b...  \n",
       "4022  x factor show gets second series tv talent sho...  \n",
       "\n",
       "[4023 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4023, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique_ID    0\n",
       "text1        0\n",
       "text2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking describtion for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4023</td>\n",
       "      <td>4023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2125</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2d metal slug offers retro fun like some drill...</td>\n",
       "      <td>brown outlines third term vision gordon brown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text1  \\\n",
       "count                                                4023   \n",
       "unique                                               2125   \n",
       "top     2d metal slug offers retro fun like some drill...   \n",
       "freq                                                    4   \n",
       "\n",
       "                                                    text2  \n",
       "count                                                4023  \n",
       "unique                                               2125  \n",
       "top     brown outlines third term vision gordon brown ...  \n",
       "freq                                                    4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text1','text2']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>labour plans maternity pay rise maternity pay ...</td>\n",
       "      <td>no seasonal lift for house market a swathe of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>high fuel costs hit us airlines two of the lar...</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>britons growing  digitally obese  gadget lover...</td>\n",
       "      <td>film star fox behind theatre bid leading actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>tsunami  to hit sri lanka banks  sri lanka s b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>nuclear dumpsite  plan attacked plans to allow...</td>\n",
       "      <td>x factor show gets second series tv talent sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text1  \\\n",
       "0     savvy searchers fail to spot ads internet sear...   \n",
       "1     millions to miss out on the net by 2025  40% o...   \n",
       "2     young debut cut short by ginepri fifteen-year-...   \n",
       "3     diageo to buy us wine firm diageo  the world s...   \n",
       "4     be careful how you code a new european directi...   \n",
       "...                                                 ...   \n",
       "4018  labour plans maternity pay rise maternity pay ...   \n",
       "4019  high fuel costs hit us airlines two of the lar...   \n",
       "4020  britons growing  digitally obese  gadget lover...   \n",
       "4021  holmes is hit by hamstring injury kelly holmes...   \n",
       "4022  nuclear dumpsite  plan attacked plans to allow...   \n",
       "\n",
       "                                                  text2  \n",
       "0     newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1     nasdaq planning $100m share sale the owner of ...  \n",
       "2     ruddock backs yapp s credentials wales coach m...  \n",
       "3     mci shares climb on takeover bid shares in us ...  \n",
       "4     media gadgets get moving pocket-sized devices ...  \n",
       "...                                                 ...  \n",
       "4018  no seasonal lift for house market a swathe of ...  \n",
       "4019  new media battle for bafta awards the bbc lead...  \n",
       "4020  film star fox behind theatre bid leading actor...  \n",
       "4021  tsunami  to hit sri lanka banks  sri lanka s b...  \n",
       "4022  x factor show gets second series tv talent sho...  \n",
       "\n",
       "[4023 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text1','text2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of dataset from text1 and text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'savvy searchers fail to spot ads internet search engine users are an odd mix of naive and sophisticated  suggests a report into search habits.  the report by the us pew research center reveals that 87% of searchers usually find what they were looking for when using a search engine. it also shows that few can spot the difference between paid-for results and organic ones. the report reveals that 84% of net users say they regularly use google  ask jeeves  msn and yahoo when online.  almost 50% of those questioned said they would trust search engines much less  if they knew information about who paid for results was being hidden. according to figures gathered by the pew researchers the average users spends about 43 minutes per month carrying out 34 separate searches and looks at 1.9 webpages for each hunt. a significant chunk of net users  36%  carry out a search at least weekly and 29% of those asked only look every few weeks. for 44% of those questioned  the information they are looking for is critical to what they are doing and is information they simply have to find.  search engine users also tend to be very loyal and once they have found a site they feel they can trust tend to stick with it. according to pew research 44% of searchers use just a single search engine  48% use two or three and a small number  7%  consult more than three sites. tony macklin  spokesman for ask jeeves  said the results reflected its own research which showed that people use different search engines because the way the sites gather information means they can provide different results for the same query. despite this liking for search sites half of those questioned said they could get the same information via other routes. a small number  17%  said they wouldn t really miss search engines if they did not exist. the remaining 33% said they could not live without search sites. more than two-thirds of those questioned  68%  said they thought that the results they were presented with were a fair and unbiased selection of the information on a topic that can be found on the net. alongside the growing sophistication of net users is a lack of awareness about paid-for results that many search engines provide alongside lists of websites found by indexing the web. of those asked  62% were unaware that someone has paid for some of the results they see when they carry out a search. only 18% of all searchers say they can tell which results are paid for and which are not. said the pew report:  this finding is ironic  since nearly half of all users say they would stop using search engines if they thought engines were not being clear about how they presented paid results.  commenting mr macklin said sponsored results must be clearly marked and though they might help with some queries user testing showed that people need to be able to spot the difference.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newcastle 2-1 bolton kieron dyer smashed home the winner to end bolton s 10-game unbeaten run.  lee bowyer put newcastle ahead when he fed stephen carr on the right flank  then sprinted into the area to power home a header from the resultant cross. wanderers hit back through stelios giannakopoulos  who ended a fluid passing move with a well-struck volley. but dyer had the last word in a game of few chances  pouncing on a loose ball after alan shearer s shot was blocked and firing into the top corner. neither side lacked urgency in the early stages of the game  with plenty of tackles flying in  but opportunities in front of goal were harder to come by. bolton keeper jussi jaaskelainen had to make two saves in quick succession midway through the first-half - keeping out shearer s low shot and dyer s close-range header - but that was the only goalmouth action of note. and it was almost out of nothing that the magpies took the lead on 35 minutes. bowyer found space with a neat turn on the half-way line and striding forward picked out carr to his right. he then continued his run and with perfect timing made his way into the box where he met carr s cross with a downward header into the far corner. bolton had produced little going forward at this point but they responded well.  they were level within six minutes thanks to a smart finish from giannakopoulos. jay-jay okocha twisted and turned on the edge of the area and after a neat exchange of passes involving kevin davies and gary speed  the greek striker found the bottom corner with a first-time strike. the magpies were opened up again before half-time as davies set giannakopoulos in space and given had to block at his near post. but the home side survived  and they should have re-taken the lead with the first meaningful attack of the second half. fernando hierro cynically chopped down dyer on the edge of the area with the midfielder clean through. but the veteran defender escaped with a booking as there were other defenders nearby  and from the resultant free-kick laurent robert curled the ball just wide. bolton were creating little going forward and they seemed content to frustrate the magpies. their strategy seemed to be working until the 69th minute. alan shearer s snap-shot was charged down and dyer reacted first to smash the ball past the despairing jaaskelainen from six yards.  - bolton boss sam allardyce  i am bitterly disappointed with the result  but i am probably more disappointed with the second-half performance.  in the first half we had put them under a lot of pressure  and our goal matched theirs in quality.  i thought it would lift us and that they might be tired after playing a lot of games  but unfortunately we were not up for the battle in the second half.  we allowed them to heap too much pressure on us  and in the end we cracked.    - newcastle boss graeme souness  we deserved the win. we had a really good second half.  bolton are a difficult side to play. you have to match them physically first but we did that  and then we played some football.  we had a slow first 45 minutes when we looked a bit tired but we got going after that. the scoreline flattered them and we could have had one or two more goals.  newcastle: given  carr  boumsong  bramble  babayaro  dyer  faye  bowyer  robert (jenas 77)  ameobi  shearer. subs not used: butt  harper  milner  hughes. goals: bowyer 35  dyer 69. bolton: jaaskelainen  hunt (fadiga 14)  n gotty  ben haim  candela  giannakopoulos  okocha (vaz te 77)  hierro (campo 64)  speed  gardner  davies. subs not used: jaidi  poole. booked: ben haim  hierro. goals: giannakopoulos 41. att: 50 430 ref: s dunn (gloucestershire).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text2'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode the phrases\n",
    "### won't -> will not\n",
    "### can't -> can not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(phrase):\n",
    "    # specific words\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general words\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_processing = []\n",
    "\n",
    "# for sentance in (df['text1'].values):\n",
    "#     sent = decode(sentance)\n",
    "#     sent = sent.replace('\\\\r', ' ')\n",
    "#     sent = sent.replace('\\\\\"', ' ')\n",
    "#     sent = sent.replace('\\\\n', ' ')\n",
    "#     sent = re.sub('[^A-Za-z0-9]+',' ',sent)\n",
    "#     sent = [wordnet.lemmatize(word) for word in sent if not word in stopwords.words('english')]\n",
    "#     sent = ' '.join(sent)\n",
    "#     pre_processing.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing of stopwords\n",
    "# Removing of Special characters\n",
    "# Lemmatizing the data set  by wordnet Lemmatizer\n",
    "# converting the data into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "preprocessed_text1 = []\n",
    "\n",
    "\n",
    "for sentance in (df['text1'].values):\n",
    "    sent = decode(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text1.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the text1 by preprocessed_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text1'] = preprocessed_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail spot ads internet search ...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions miss net 2025 40 uk population still ...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy us wine firm diageo world biggest s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail spot ads internet search ...   \n",
       "1          1  millions miss net 2025 40 uk population still ...   \n",
       "2          2  young debut cut short ginepri fifteen year old...   \n",
       "3          3  diageo buy us wine firm diageo world biggest s...   \n",
       "4          4  careful code new european directive could put ...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1  nasdaq planning $100m share sale the owner of ...  \n",
       "2  ruddock backs yapp s credentials wales coach m...  \n",
       "3  mci shares climb on takeover bid shares in us ...  \n",
       "4  media gadgets get moving pocket-sized devices ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'millions miss net 2025 40 uk population still without internet access home says study around 23 million britons miss wide range essential services education medical information predicts report telecoms giant bt compares 27 million 50 uk currently online idea digital divide evaporate time wishful thinking report concludes study calls government telecoms industry come new ways lure bypassed digital revolution although percentage britons without home access fallen slightly remain digital refuseniks miss report suggests everyday tasks move online offline services become less comprehensive divide become obvious burdensome got net access predicts gap nets nots much talked predictions divide affect future generations less discussed bt set predict future patterns based current information taking account way technology changing optimists predict convergence emergence user friendly technology bridge digital divide could way mark report suggests internet access devices tends something taken already said adrian hosford director corporate responsibility bt costs internet access fallen dramatically coverage remote areas vastly improved last year real barrier remains psychological hard rump nots engaging net motivation skills perceive benefits said mr hosford disadvantaged groups likely remain among low income families older generation disabled low incomes account quarter digital nots disabled make 16 elderly nearly third 2025 report forecasts organisations bt responsibility help tackle problem said mr hosford telco seen positive results everybody online project offers internet access people eight deprived communities around britain one area cornwall high levels unemployment online training helped people rewrite cvs learn skills get new jobs explained mr hosford grassroot activity addressing specific needs individual communities essential problem digital divide overcome said address problem get lot worse people find difficult find jobs education opportunities limited simply able keep society said alliance digital inclusion independent body members drawn government industry voluntary sector recently set tackle issues faced digital refuseniks'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text2 = []\n",
    "\n",
    "for sentance in (df['text2'].values):\n",
    "    sent = decode(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "   \n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text2.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newcastle 2 1 bolton kieron dyer smashed home winner end bolton 10 game unbeaten run lee bowyer put newcastle ahead fed stephen carr right flank sprinted area power home header resultant cross wanderers hit back stelios giannakopoulos ended fluid passing move well struck volley dyer last word game chances pouncing loose ball alan shearer shot blocked firing top corner neither side lacked urgency early stages game plenty tackles flying opportunities front goal harder come bolton keeper jussi jaaskelainen make two saves quick succession midway first half keeping shearer low shot dyer close range header goalmouth action note almost nothing magpies took lead 35 minutes bowyer found space neat turn half way line striding forward picked carr right continued run perfect timing made way box met carr cross downward header far corner bolton produced little going forward point responded well level within six minutes thanks smart finish giannakopoulos jay jay okocha twisted turned edge area neat exchange passes involving kevin davies gary speed greek striker found bottom corner first time strike magpies opened half time davies set giannakopoulos space given block near post home side survived taken lead first meaningful attack second half fernando hierro cynically chopped dyer edge area midfielder clean veteran defender escaped booking defenders nearby resultant free kick laurent robert curled ball wide bolton creating little going forward seemed content frustrate magpies strategy seemed working 69th minute alan shearer snap shot charged dyer reacted first smash ball past despairing jaaskelainen six yards bolton boss sam allardyce bitterly disappointed result probably disappointed second half performance first half put lot pressure goal matched quality thought would lift us might tired playing lot games unfortunately battle second half allowed heap much pressure us end cracked newcastle boss graeme souness deserved win really good second half bolton difficult side play match physically first played football slow first 45 minutes looked bit tired got going scoreline flattered could one two goals newcastle given carr boumsong bramble babayaro dyer faye bowyer robert jenas 77 ameobi shearer subs used butt harper milner hughes goals bowyer 35 dyer 69 bolton jaaskelainen hunt fadiga 14 n gotty ben haim candela giannakopoulos okocha vaz te 77 hierro campo 64 speed gardner davies subs used jaidi poole booked ben haim hierro goals giannakopoulos 41 att 50 430 ref dunn gloucestershire']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the text2 with preprocessed_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text2'] = preprocessed_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail spot ads internet search ...</td>\n",
       "      <td>newcastle 2 1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions miss net 2025 40 uk population still ...</td>\n",
       "      <td>nasdaq planning 100m share sale owner technolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock backs yapp credentials wales coach mik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy us wine firm diageo world biggest s...</td>\n",
       "      <td>mci shares climb takeover bid shares us phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>media gadgets get moving pocket sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail spot ads internet search ...   \n",
       "1          1  millions miss net 2025 40 uk population still ...   \n",
       "2          2  young debut cut short ginepri fifteen year old...   \n",
       "3          3  diageo buy us wine firm diageo world biggest s...   \n",
       "4          4  careful code new european directive could put ...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle 2 1 bolton kieron dyer smashed home ...  \n",
       "1  nasdaq planning 100m share sale owner technolo...  \n",
       "2  ruddock backs yapp credentials wales coach mik...  \n",
       "3  mci shares climb takeover bid shares us phone ...  \n",
       "4  media gadgets get moving pocket sized devices ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word embeddings are low dimensional vectors obtained by training a neural network on a large corpus to predict a word given a context (Continuous Bag Of Words model) or to predict the context given a word (skip gram model). The context is a window of surrounding words. Pre-trained word embeddings are also available in the word2vec code.google page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre trained google news vectors after downloading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Similarity text between text1 and text2 using n_similarity which compares the cosine similarity between two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this it will check whether word in text1 and text 2 present in google news vector library\n",
    "# if word not present it will remove the word otherwise it will compares the simliarityb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating dataset as Unsupervised learning problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means high similarity\n",
    "# 1 low similarity\n",
    "\n",
    "similarity=[]\n",
    "\n",
    "for ind in df.index:\n",
    "    s1=df['text1'][ind]\n",
    "    s2=df['text2'][ind]\n",
    "    \n",
    "    if s1 == s2:\n",
    "        similarity.append(0.0)\n",
    "    else:\n",
    "        s1words = word_tokenizer(s1)\n",
    "        s2words = word_tokenizer(s2)\n",
    "#  vocabulary considered in the word embeddings       \n",
    "        vocab = model.vocab\n",
    "        \n",
    "        if len(s1words and s2words)==0:\n",
    "            similarity.append(1.0)\n",
    "        else:\n",
    "# remove the sentence words not found in the vocab        \n",
    "            for word in s1words.copy():\n",
    "                if(word not in vocab):\n",
    "                    \n",
    "                    s1words.remove(word)\n",
    "            \n",
    "            for word in s2words.copy():\n",
    "                if(word not in vocab):\n",
    "                    s2words.remove(word)\n",
    "                    \n",
    "            similarity.append((1-model.n_similarity(s1words,s2words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the unique id with similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.389471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.292066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.272890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.184955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.171677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>0.228033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>0.403985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>0.292103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>0.437650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>0.361156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID  Similarity_score\n",
       "0             0          0.389471\n",
       "1             1          0.292066\n",
       "2             2          0.272890\n",
       "3             3          0.184955\n",
       "4             4          0.171677\n",
       "...         ...               ...\n",
       "4018       4018          0.228033\n",
       "4019       4019          0.403985\n",
       "4020       4020          0.292103\n",
       "4021       4021          0.437650\n",
       "4022       4022          0.361156\n",
       "\n",
       "[4023 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score=pd.DataFrame({'Unique_ID':df.Unique_ID,\n",
    "                         'Similarity_score':similarity\n",
    "                         })\n",
    "final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Dataframe In csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score.to_csv('D:\\\\python\\\\Final_Score.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
